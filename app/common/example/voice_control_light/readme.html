<h1>Voice Control Light</h1>
<p>This application uses TensorFlow Lite for Microcontrollers to detect the spoken words &quot;on&quot; and &quot;off&quot; from audio data recorded on the microphone. The detected keywords are used to control an LED on the board. </p>
<p>Audio is sampled continuously from the microphone at a rate of 8kHz. The frequency components are then extracted by calculating the FFT on short segments of audio as they become available. This results is an array of filterbanks, representing the frequency components of the past second of audio. The process audio data is passed into a neural network at an interval of 300ms. If the model detects that either of the keywords &quot;on&quot; or &quot;off&quot; were spoken, LED0 will toggle accordingly.</p>
<p>The application is based on TensorFlow Lite Micro&#39;s example application, <strong><a href='https://github.com/tensorflow/tflite-micro/tree/3e190e5389be49c94475e509452bdae245bd4fa6/tensorflow/lite/micro/examples/micro_speech'>micro speech</a></strong>.</p>
<h2>Model  </h2>
<p>The neural network model has been trained to identify the two keywords &quot;on&quot; and &quot;off&quot; from preprocessed audio data. When neither words are recognized, the model will classify the input as either &quot;unknown&quot; or &quot;background&quot;. </p>
<p>The model takes an array of filterbanks as input and outputs a vector with each value corresponding to the probability that the input belongs to each of the categories (&quot;on&quot;, &quot;off&quot;, &quot;unknown&quot;, &quot;background&quot;). The architecture of the model is visualized below (_using [Netron</p>
